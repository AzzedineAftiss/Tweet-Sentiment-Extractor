{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-22T10:49:32.684046Z","iopub.execute_input":"2022-09-22T10:49:32.684995Z","iopub.status.idle":"2022-09-22T10:49:43.091206Z","shell.execute_reply.started":"2022-09-22T10:49:32.684957Z","shell.execute_reply":"2022-09-22T10:49:43.089987Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom time import time\nfrom torch.utils.data import DataLoader\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:24.478939Z","iopub.execute_input":"2022-09-22T10:50:24.479333Z","iopub.status.idle":"2022-09-22T10:50:24.486368Z","shell.execute_reply.started":"2022-09-22T10:50:24.479298Z","shell.execute_reply":"2022-09-22T10:50:24.485173Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\ntrain = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:25.237660Z","iopub.execute_input":"2022-09-22T10:50:25.238337Z","iopub.status.idle":"2022-09-22T10:50:25.315269Z","shell.execute_reply.started":"2022-09-22T10:50:25.238301Z","shell.execute_reply":"2022-09-22T10:50:25.314291Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nle = LabelEncoder()\ntrain_y = le.fit_transform(train[\"sentiment\"].values)\ntrain[\"label\"] = train_y","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:25.536439Z","iopub.execute_input":"2022-09-22T10:50:25.537155Z","iopub.status.idle":"2022-09-22T10:50:25.549543Z","shell.execute_reply.started":"2022-09-22T10:50:25.537116Z","shell.execute_reply":"2022-09-22T10:50:25.548417Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:26.285876Z","iopub.execute_input":"2022-09-22T10:50:26.286271Z","iopub.status.idle":"2022-09-22T10:50:26.306005Z","shell.execute_reply.started":"2022-09-22T10:50:26.286239Z","shell.execute_reply":"2022-09-22T10:50:26.304572Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 27481 entries, 0 to 27480\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   textID         27481 non-null  object\n 1   text           27480 non-null  object\n 2   selected_text  27480 non-null  object\n 3   sentiment      27481 non-null  object\n 4   label          27481 non-null  int64 \ndtypes: int64(1), object(4)\nmemory usage: 1.0+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"le.classes_","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:26.376896Z","iopub.execute_input":"2022-09-22T10:50:26.377194Z","iopub.status.idle":"2022-09-22T10:50:26.383811Z","shell.execute_reply.started":"2022-09-22T10:50:26.377166Z","shell.execute_reply":"2022-09-22T10:50:26.382758Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array(['negative', 'neutral', 'positive'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"train.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:26.581410Z","iopub.execute_input":"2022-09-22T10:50:26.582090Z","iopub.status.idle":"2022-09-22T10:50:26.598954Z","shell.execute_reply.started":"2022-09-22T10:50:26.582054Z","shell.execute_reply":"2022-09-22T10:50:26.598046Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name = \"microsoft/deberta-base\"\n    batch_size = 2\n    apex = True\n    debug = True\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    max_length = 512\n    dropout = 0.1\n    folds = 5\n    gradient_accumulation = 5\n    scheduler = \"cosine\"\n    num_warmup_steps=0\n    apex = True\n    epochs = 10","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:27.521534Z","iopub.execute_input":"2022-09-22T10:50:27.522145Z","iopub.status.idle":"2022-09-22T10:50:27.528775Z","shell.execute_reply.started":"2022-09-22T10:50:27.522109Z","shell.execute_reply":"2022-09-22T10:50:27.527335Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:27.685396Z","iopub.execute_input":"2022-09-22T10:50:27.685759Z","iopub.status.idle":"2022-09-22T10:50:27.692357Z","shell.execute_reply.started":"2022-09-22T10:50:27.685726Z","shell.execute_reply":"2022-09-22T10:50:27.691249Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def get_scheduler(CFG, optimizer, num_training_steps):\n    if CFG.scheduler == \"cosine\":\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_training_steps)\n    elif CFG.scheduler == \"linear\":\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_training_steps)\n    return scheduler\n        ","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:28.914648Z","iopub.execute_input":"2022-09-22T10:50:28.915216Z","iopub.status.idle":"2022-09-22T10:50:28.921943Z","shell.execute_reply.started":"2022-09-22T10:50:28.915182Z","shell.execute_reply":"2022-09-22T10:50:28.920745Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:50:29.979267Z","iopub.execute_input":"2022-09-22T10:50:29.980258Z","iopub.status.idle":"2022-09-22T10:50:41.331449Z","shell.execute_reply.started":"2022-09-22T10:50:29.980212Z","shell.execute_reply":"2022-09-22T10:50:41.330424Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(tweet, sentiment, selected_text, CFG):\n  \n  selected_text = \" \" + \" \".join(str(selected_text).split())\n  tweet = \" \" + \" \".join(str(tweet).split())\n\n\n  tokenized_text = CFG.tokenizer(tweet, add_special_tokens=False, padding='max_length', max_length=100, truncation=True, return_offsets_mapping=True)\n\n\n  len_st = len(selected_text) - 1\n  idx0 = None\n  idx1 = None\n  for ind in (start_idx for start_idx, e in enumerate(tweet) if e == selected_text[1]):\n    if \" \"+tweet[ind:ind+len_st] == selected_text:\n      idx0 = ind\n      idx1 = ind+len_st-1\n      break\n\n  char_targets = [0]*len(tweet)\n  for idx in range(idx0, idx1+1):\n    char_targets[idx] = 1\n  \n\n  target_tokens_idx = []\n  for i, (start_indx, end_indx) in enumerate(tokenized_text[\"offset_mapping\"]):\n    if sum(char_targets[start_indx:end_indx]) > 0:\n      target_tokens_idx.append(i)\n  \n\n  start_indx = target_tokens_idx[0]\n  end_indx = target_tokens_idx[-1]\n\n\n  sentiments_id = {\n    'positive': 1313,\n    'negative': 2430,\n    'neutral': 7974\n  }\n\n  tokenized_text.keys()\n  input_ids = [0] + [sentiments_id[sentiment]] + [2] + [2] + tokenized_text[\"input_ids\"] + [2]\n  token_type_ids = [0] * 4 + tokenized_text[\"token_type_ids\"] + [0]\n  attention_mask = [1] * 4 + tokenized_text[\"attention_mask\"] + [0]\n  offset_mapping = [(0,0)] *4 + tokenized_text[\"offset_mapping\"] + [(0,0)]\n\n\n  data =  {\n      \"input_ids\" : input_ids,\n      \"token_type_ids\": token_type_ids,\n      \"attention_mask\": attention_mask,\n      \"offset_mapping\": offset_mapping,\n      \"tweet\" : tweet,\n      \"selected_text\": selected_text,\n      \"start_token_indx\": start_indx,\n      \"end_token_indx\": end_indx,\n      \"sentiment\" : sentiment\n\n  }\n\n  return data\n","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:06.701073Z","iopub.execute_input":"2022-09-22T10:51:06.701457Z","iopub.status.idle":"2022-09-22T10:51:06.713617Z","shell.execute_reply.started":"2022-09-22T10:51:06.701423Z","shell.execute_reply":"2022-09-22T10:51:06.712527Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from pandas.io.xml import preprocess_data\nclass TextDataset(torch.utils.data.Dataset):\n\n  def __init__(self, train_df, CFG):\n\n    self.CFG = CFG\n    self.texts = train_df[\"text\"].values\n    self.sentiments = train_df[\"sentiment\"].values\n    self.selected_texts = train_df[\"selected_text\"].values\n    self.labels = train_df[\"label\"].values\n  \n  def __getitem__(self, idx):\n    tweet = self.texts[idx]\n    sentiment = self.sentiments[idx]\n    selected_text = self.selected_texts[idx]\n    label = self.labels[idx]\n    preprocessed_text = prepare_input(tweet, sentiment, selected_text, self.CFG)\n\n    preprocessed_text =  {\n        \"input_ids\" : torch.tensor(preprocessed_text[\"input_ids\"], dtype=torch.long),\n        \"token_type_ids\": torch.tensor(preprocessed_text[\"token_type_ids\"], dtype=torch.long),\n        \"attention_mask\": torch.tensor(preprocessed_text[\"attention_mask\"], dtype=torch.long),\n        \"offset_mapping\": torch.tensor(preprocessed_text[\"offset_mapping\"], dtype=torch.long),\n        \"origin_tweet\" : preprocessed_text[\"tweet\"],\n        \"origin_selected_text\": preprocessed_text[\"selected_text\"],\n        \"start_token_indx\": torch.tensor(preprocessed_text[\"start_token_indx\"], dtype=torch.long),\n        \"end_token_indx\": torch.tensor(preprocessed_text[\"end_token_indx\"], dtype=torch.long),\n        \"sentiment\": preprocessed_text[\"sentiment\"],\n        \"label\": torch.tensor(label, dtype=torch.int64)\n      }\n\n    return preprocessed_text\n  \n  def __len__(self):\n    return len(self.texts)","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:07.056351Z","iopub.execute_input":"2022-09-22T10:51:07.057053Z","iopub.status.idle":"2022-09-22T10:51:07.067744Z","shell.execute_reply.started":"2022-09-22T10:51:07.057009Z","shell.execute_reply":"2022-09-22T10:51:07.066693Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class TweetModel(torch.nn.Module):\n\n  def __init__(self, CFG):\n    super(TweetModel, self).__init__()\n    self.config = AutoConfig.from_pretrained(CFG.model_name)\n    self.model = AutoModel.from_pretrained(CFG.model_name)\n    self.dropout = nn.Dropout(CFG.dropout)\n    self.fc = nn.Linear(self.config.hidden_size, 2)\n    self.cls = nn.Linear(self.config.hidden_size, 3)\n  def forward(self, data):\n\n    features = self.model(input_ids = data[\"input_ids\"], attention_mask=data['attention_mask'],token_type_ids=data[\"token_type_ids\"]) #\n    last_hidden_state = self.dropout(features[0])\n\n    out = self.fc(last_hidden_state)\n    start_logits, end_logits = torch.split(out, 1, dim=2)\n    start_logits = start_logits.squeeze(-1)\n    end_logits = end_logits.squeeze(-1)\n    sentence_representation = last_hidden_state[:,0,:]\n    class_logit = self.cls(sentence_representation)\n    return start_logits, end_logits, class_logit\n","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:07.305527Z","iopub.execute_input":"2022-09-22T10:51:07.306233Z","iopub.status.idle":"2022-09-22T10:51:07.314949Z","shell.execute_reply.started":"2022-09-22T10:51:07.306196Z","shell.execute_reply":"2022-09-22T10:51:07.313827Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n  def __init__(self):\n    self.reset()\n  \n  def reset(self):\n    self.val = 0\n    self.sum = 0\n    self.avg = 0\n    self.count = 0\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val*n\n    self.count += n\n    self.avg = self.sum / self.count\n","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:07.552385Z","iopub.execute_input":"2022-09-22T10:51:07.552758Z","iopub.status.idle":"2022-09-22T10:51:07.559727Z","shell.execute_reply.started":"2022-09-22T10:51:07.552726Z","shell.execute_reply":"2022-09-22T10:51:07.558580Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:07.672289Z","iopub.execute_input":"2022-09-22T10:51:07.672634Z","iopub.status.idle":"2022-09-22T10:51:07.678640Z","shell.execute_reply.started":"2022-09-22T10:51:07.672599Z","shell.execute_reply":"2022-09-22T10:51:07.677543Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def format_prediction(original_tweet, target_string, sentiment_val, offset, predicted_start_token__, predicted_end_token__):\n\n  if predicted_start_token__ > predicted_end_token__:\n    predicted_end_token__ = predicted_start_token__\n  \n  predicted_selected_text = \" \"\n  for ix in range(predicted_start_token__, predicted_end_token__+1):\n    start_char = offset[ix][0]\n    end_char = offset[ix][1]\n    predicted_selected_text += original_tweet[start_char:end_char]\n    if (ix+1) < len(offset) + 1 and offset[ix][1] < offset[ix][0]:\n      predicted_selected_text += \" \"\n  if sentiment_val == \"neutral\" or len(original_tweet.split(\" \")) <2:\n    predicted_selected_text = original_tweet\n\n  jac_score = jaccard(target_string.strip(), predicted_selected_text.strip())\n  return jac_score, predicted_selected_text, predicted_selected_text","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:08.112552Z","iopub.execute_input":"2022-09-22T10:51:08.113267Z","iopub.status.idle":"2022-09-22T10:51:08.120551Z","shell.execute_reply.started":"2022-09-22T10:51:08.113230Z","shell.execute_reply":"2022-09-22T10:51:08.119504Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from itertools import starmap\ndef training_epoch(epoch, model, optimizer, criterion, train_dl, scheduler):\n    scaler = torch.cuda.amp.GradScaler()\n    model.train()\n    loss_val = AverageMeter()\n    \n    for step, data in enumerate(train_dl):\n        input_ids = data[\"input_ids\"].to(CFG.device)\n        token_type_ids = data[\"token_type_ids\"].to(CFG.device)\n        attention_mask = data[\"attention_mask\"].to(CFG.device)\n        start_token_indx = data[\"start_token_indx\"].to(CFG.device)\n        end_token_indx = data[\"end_token_indx\"].to(CFG.device)\n        label = data[\"label\"].to(CFG.device)\n        batch_size = end_token_indx.shape[0]\n#         print(f\"label type : {label.type()}\")\n        with torch.cuda.amp.autocast():\n            predicted_start_token, predicted_end_token, class_logit = model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask})\n\n            start_loss = criterion(predicted_start_token, start_token_indx)\n            end_loss = criterion(predicted_end_token, end_token_indx)\n#             print(f\"class_logit type : {class_logit.type()}\")\n#             print(f\"class_logit shape : {class_logit.shape}\")\n#             print(f\"class_logit shape : {label.shape}\")\n            cls_loss = criterion(class_logit, label)\n            total_loss = start_loss + end_loss + cls_loss\n\n        loss_val.update(total_loss.item(), batch_size)\n        if CFG.gradient_accumulation > 0:\n            total_loss = total_loss/ CFG.gradient_accumulation\n        \n        if (step+1) % CFG.gradient_accumulation == 0:\n            scaler.scale(total_loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            if CFG.scheduler:\n                scheduler.step()\n\n    return loss_val.avg\n\n\n\n\ndef valid_epoch(epoch, model, criterion, valid_dl):\n    model.eval()\n    loss_val = AverageMeter()\n    jaccards = AverageMeter()\n    predicted_selected_texts = []\n    for data in valid_dl:\n        input_ids = data[\"input_ids\"].to(CFG.device)\n        token_type_ids = data[\"token_type_ids\"].to(CFG.device)\n        attention_mask = data[\"attention_mask\"].to(CFG.device)\n        start_token_indx = data[\"start_token_indx\"].to(CFG.device)\n        end_token_indx = data[\"end_token_indx\"].to(CFG.device)\n        offset_mapping = data[\"offset_mapping\"]\n        origin_tweet = data[\"origin_tweet\"]\n        origin_selected_text = data[\"origin_selected_text\"]\n        sentiment_val = data[\"sentiment\"]\n        label = data[\"label\"].to(CFG.device)\n        batch_size = end_token_indx.shape[0]\n    \n        with torch.no_grad():\n            predicted_start_token, predicted_end_token, class_logit = model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask})\n\n        start_loss = criterion(predicted_start_token, start_token_indx)\n        end_loss = criterion(predicted_end_token, end_token_indx)\n        cls_loss = criterion(class_logit, label)\n        total_loss = start_loss + end_loss + cls_loss\n\n        predicted_start_token__ = torch.softmax(predicted_start_token, dim=1).cpu().detach().numpy()\n        predicted_end_token__ = torch.softmax(predicted_end_token, dim=1).cpu().detach().numpy()\n        predicted_label = torch.softmax(class_logit, dim=1).cpu().detach().numpy()\n\n        jaccard_scores = []\n        for px, tweet in enumerate(origin_tweet):\n            jacard, _, predicted_selected_text = format_prediction(tweet, origin_selected_text[px], sentiment_val[px], offset_mapping[px], np.argmax(predicted_start_token__[px, :]), np.argmax(predicted_end_token__[px, :]))\n            jaccard_scores.append(jacard)\n            predicted_selected_texts.append(predicted_selected_text)\n\n        loss_val.update(total_loss.item(), batch_size)\n        jaccards.update(np.mean(jaccard_scores), batch_size)\n    \n    predicted_selected_texts = np.array(predicted_selected_texts)\n    return loss_val.avg, jaccards.avg, predicted_selected_texts","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:52:52.459463Z","iopub.execute_input":"2022-09-22T10:52:52.459833Z","iopub.status.idle":"2022-09-22T10:52:52.477985Z","shell.execute_reply.started":"2022-09-22T10:52:52.459801Z","shell.execute_reply":"2022-09-22T10:52:52.476874Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def train_loop(fold, train, CFG):\n    train_folds = train[train[\"fold\"] != fold].reset_index()\n    valid_fold = train[train[\"fold\"] == fold].reset_index()\n\n    train_dataset = TextDataset(train_folds, CFG)\n    valid_dataset = TextDataset(valid_fold, CFG)\n\n    train_dl = DataLoader(train_dataset, batch_size=CFG.batch_size)\n    valid_dl = DataLoader(valid_dataset, batch_size=CFG.batch_size)\n        \n    model = TweetModel(CFG).to(CFG.device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n    num_training_steps = int(len(train_folds))/CFG.batch_size * CFG.epochs\n    scheduler = get_scheduler(CFG, optimizer, num_training_steps) \n    \n    best_score = -np.Inf\n    for epoch in range(CFG.epochs):\n        start_time = time()\n        avg_train_loss = training_epoch(epoch, model, optimizer, criterion, train_dl, scheduler)\n        print(f\"avg_train_loss = {avg_train_loss}\")\n        avg_val_loss, score, predicted_selected_text = valid_epoch(epoch, model, criterion, valid_dl)\n        print(f\"avg_val_loss = {avg_val_loss}\")\n\n        end_time = time()\n        elapsed = end_time - start_time\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_train_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        if score > best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({\n                'epoch': epoch+1,\n                'model_state_dict': model.state_dict(),\n                'prediction_selected_texts': predicted_selected_text,\n            }, OUTPUT_DIR+f\"model_fold{fold}.pth\")\n  \n    torch.cuda.empty_cache()\n\n    valid_fold[\"predicted_selected_text\"] = predicted_selected_text\n    return valid_fold","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:52:54.830789Z","iopub.execute_input":"2022-09-22T10:52:54.831147Z","iopub.status.idle":"2022-09-22T10:52:54.842205Z","shell.execute_reply.started":"2022-09-22T10:52:54.831118Z","shell.execute_reply":"2022-09-22T10:52:54.841143Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# # For Debugging\n# CFG.debug  = True\n# if CFG.debug == True:\n#     CFG.folds = 5\n#     train = train.sample(100, replace=True).reset_index(drop=True)\n# train.shape\n# train = train.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:09.115374Z","iopub.execute_input":"2022-09-22T10:51:09.116288Z","iopub.status.idle":"2022-09-22T10:51:09.127505Z","shell.execute_reply.started":"2022-09-22T10:51:09.116250Z","shell.execute_reply":"2022-09-22T10:51:09.126264Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=None, shuffle=True)\nfor fold , (train_index, valid_index) in enumerate(kf.split(train[\"text\"])):\n    train.loc[valid_index, \"fold\"] = fold","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:15.481238Z","iopub.execute_input":"2022-09-22T10:51:15.481631Z","iopub.status.idle":"2022-09-22T10:51:15.493715Z","shell.execute_reply.started":"2022-09-22T10:51:15.481569Z","shell.execute_reply":"2022-09-22T10:51:15.492425Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"logger = getLogger(__name__)\nlogger.setLevel(INFO)","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:21.674070Z","iopub.execute_input":"2022-09-22T10:51:21.674428Z","iopub.status.idle":"2022-09-22T10:51:21.681033Z","shell.execute_reply.started":"2022-09-22T10:51:21.674395Z","shell.execute_reply":"2022-09-22T10:51:21.679835Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"OUTPUT_DIR = \"./\"\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    \n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n#     handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:51:23.095448Z","iopub.execute_input":"2022-09-22T10:51:23.096200Z","iopub.status.idle":"2022-09-22T10:51:23.102511Z","shell.execute_reply.started":"2022-09-22T10:51:23.096158Z","shell.execute_reply":"2022-09-22T10:51:23.101244Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"LOGGER = get_logger()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:47:21.815959Z","iopub.execute_input":"2022-09-22T10:47:21.818354Z","iopub.status.idle":"2022-09-22T10:47:21.823846Z","shell.execute_reply.started":"2022-09-22T10:47:21.818316Z","shell.execute_reply":"2022-09-22T10:47:21.822975Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\n\n\n# oof_df = pd.DataFrame()\n# for fold in range(CFG.folds):\n#     _oof_df = train_loop(fold, train, CFG)\n#     oof_df = pd.concat([oof_df, _oof_df])\n#     LOGGER.info(f\"Training at Fold {fold} has ended !!!\")\n\n# oof_df.to_pickle(\"oof_df.pickle\")","metadata":{"execution":{"iopub.status.busy":"2022-09-22T12:00:47.546098Z","iopub.execute_input":"2022-09-22T12:00:47.546597Z","iopub.status.idle":"2022-09-22T12:00:47.575579Z","shell.execute_reply.started":"2022-09-22T12:00:47.546477Z","shell.execute_reply":"2022-09-22T12:00:47.574181Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def prepare_input_inference(tweet, CFG):\n    \n    tweet = \" \" + \" \".join(str(tweet).split())\n\n\n    tokenized_text = CFG.tokenizer(tweet, add_special_tokens=False, padding='max_length', max_length=100, truncation=True, return_offsets_mapping=True)\n\n\n  \n  \n\n \n   \n\n    input_ids = [0, 0] + [2] + [2] + tokenized_text[\"input_ids\"] + [2]\n    token_type_ids = [0] * 4 + tokenized_text[\"token_type_ids\"] + [0]\n    attention_mask = [1] * 4 + tokenized_text[\"attention_mask\"] + [0]\n    offset_mapping = [(0,0)] *4 + tokenized_text[\"offset_mapping\"] + [(0,0)]\n\n\n    data =  {\n        \"input_ids\" : torch.tensor([input_ids], dtype=torch.long).to(CFG.device),\n        \"token_type_ids\": torch.tensor([token_type_ids], dtype=torch.long).to(CFG.device),\n        \"attention_mask\": torch.tensor([attention_mask], dtype=torch.long).to(CFG.device),\n        \"offset_mapping\": torch.tensor([offset_mapping], dtype=torch.long).to(CFG.device)\n        \n    }\n\n    return data, tweet\n","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:58:01.090502Z","iopub.execute_input":"2022-09-22T10:58:01.091528Z","iopub.status.idle":"2022-09-22T10:58:01.102022Z","shell.execute_reply.started":"2022-09-22T10:58:01.091491Z","shell.execute_reply":"2022-09-22T10:58:01.100981Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def format_inference(original_tweet, offset, predicted_start_token__, predicted_end_token__, predicted_label):\n    if predicted_start_token__ > predicted_end_token__:\n        predicted_end_token__ = predicted_start_token__\n    offset = offset.squeeze()\n    predicted_selected_text = \" \"\n    \n    \n    \n    for ix in range(predicted_start_token__, predicted_end_token__+1):\n        start_char = offset[ix][0]\n        end_char = offset[ix][1]\n        predicted_selected_text += original_tweet[start_char:end_char]\n        if (ix+1) < len(offset) + 1 and offset[ix][1] < offset[ix][0]:\n            predicted_selected_text += \" \"\n    \n    sentiment_val = le.classes_[predicted_label]\n    if sentiment_val == \"neutral\" or len(original_tweet.split(\" \")) <2:\n        predicted_selected_text = original_tweet\n    \n    return predicted_selected_text, sentiment_val","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:58:06.592104Z","iopub.execute_input":"2022-09-22T10:58:06.592453Z","iopub.status.idle":"2022-09-22T10:58:06.600054Z","shell.execute_reply.started":"2022-09-22T10:58:06.592423Z","shell.execute_reply":"2022-09-22T10:58:06.598868Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"tweet = \"Recession hit Veronique Branquinho, she has to quit her company, such a shame!\"","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:58:10.376712Z","iopub.execute_input":"2022-09-22T10:58:10.377189Z","iopub.status.idle":"2022-09-22T10:58:10.382898Z","shell.execute_reply.started":"2022-09-22T10:58:10.377144Z","shell.execute_reply":"2022-09-22T10:58:10.381593Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"data,tweet = prepare_input_inference(tweet, CFG)","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:58:17.818225Z","iopub.execute_input":"2022-09-22T10:58:17.818614Z","iopub.status.idle":"2022-09-22T10:58:17.824832Z","shell.execute_reply.started":"2022-09-22T10:58:17.818560Z","shell.execute_reply":"2022-09-22T10:58:17.823645Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def inference(data, tweet):\n    model = TweetModel(CFG).to(CFG.device)\n    start_index_logits = []\n    end_index_logits = []\n    cls_logits = []\n    tweet = test.iloc[2][\"text\"]\n\n    \n    for fold in range(CFG.folds):\n        path = f\"./model_fold{fold}.pth\"\n        checkpoint = torch.load(path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        predicted_start_token, predicted_end_token, cls_logit = model(data)\n        start_index_logits.append(predicted_start_token)\n        end_index_logits.append(predicted_end_token)\n        cls_logits.append(cls_logit)\n\n    start_index_logit = torch.mean(torch.stack(start_index_logits), dim=0)\n    end_index_logit = torch.mean(torch.stack(end_index_logits), dim=0)\n    cls_logit = torch.mean(torch.stack(cls_logits), dim=0)\n\n    predicted_start_token__ = torch.softmax(start_index_logit, dim=1).cpu().detach().numpy()\n    predicted_end_token__ = torch.softmax(end_index_logit, dim=1).cpu().detach().numpy()\n    predicted_label = torch.softmax(cls_logit, dim=1).cpu().detach().numpy()\n\n    predicted_selected_text = format_inference(tweet, data[\"offset_mapping\"], np.argmax(predicted_start_token__[0, :]), np.argmax(predicted_end_token__[0, :]), np.argmax(predicted_label[0, :]))\n\n\n    return predicted_selected_text\n\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:58:23.706672Z","iopub.execute_input":"2022-09-22T10:58:23.707361Z","iopub.status.idle":"2022-09-22T10:58:23.718247Z","shell.execute_reply.started":"2022-09-22T10:58:23.707323Z","shell.execute_reply":"2022-09-22T10:58:23.716953Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"predicted_selected_text, sentiment_val = inference(data, tweet)","metadata":{"execution":{"iopub.status.busy":"2022-09-22T10:58:28.061823Z","iopub.execute_input":"2022-09-22T10:58:28.062260Z","iopub.status.idle":"2022-09-22T10:58:34.910339Z","shell.execute_reply.started":"2022-09-22T10:58:28.062218Z","shell.execute_reply":"2022-09-22T10:58:34.909376Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"original tweet : {tweet}\")\nprint(f\"sentiment of the tweet : {sentiment_val}\")\nprint(f\"selected text  :{predicted_selected_text}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-22T11:05:26.720441Z","iopub.execute_input":"2022-09-22T11:05:26.721135Z","iopub.status.idle":"2022-09-22T11:05:26.726193Z","shell.execute_reply.started":"2022-09-22T11:05:26.721095Z","shell.execute_reply":"2022-09-22T11:05:26.725071Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"original tweet :  Recession hit Veronique Branquinho, she has to quit her company, such a shame!\nsentiment of the tweet : positive\nselected text  : shame!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}